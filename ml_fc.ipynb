{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "950f1da7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae86e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansatz import Ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799fef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be876c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d0edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ansatz = Ansatz(n_qubits = 2, n_layers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6f9de",
   "metadata": {},
   "source": [
    "# Architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLinearModule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, layer_sizes, activation, out_activation):\n",
    "        \n",
    "        '''\n",
    "        layers: list containing the size of the linear layers between input and output\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # add first and last layers\n",
    "        layer_sizes.insert(0, in_dim)\n",
    "        layer_sizes.append(out_dim*2)\n",
    "        \n",
    "        num_layers = len(layer_sizes)-1\n",
    "        layers_list = []\n",
    "        for i in range(num_layers):\n",
    "            if i!=0:\n",
    "                layers_list.append(activation())\n",
    "            layers_list.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            # layers_list.append(nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "            # for some reason batchnorm ruins everything, maybe add on the first few layers?\n",
    "        layers_list.append(out_activation())\n",
    "        \n",
    "        self.f = nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sin_theta, cos_theta = torch.split(self.f(x), self.out_dim, 1)\n",
    "        \n",
    "        return torch.atan2(sin_theta, cos_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a739e",
   "metadata": {},
   "source": [
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d81de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnsatzDataset(torch.utils.data.TensorDataset):\n",
    "    def __init__(self, n_pairs):\n",
    "        self.psi, self.theta = my_ansatz.generate_pairs(n_pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.psi[idx], self.theta[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e695bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AnsatzDataset(1024*100)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=512)\n",
    "\n",
    "test_data = AnsatzDataset(1024)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71294890",
   "metadata": {},
   "source": [
    "# Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78199202",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 2**my_ansatz.n_qubits\n",
    "out_dim = my_ansatz.n_theta\n",
    "out_activation = nn.Tanh\n",
    "activation = nn.LeakyReLU\n",
    "layer_sizes = [(in_dim+out_dim)*2+40]*8\n",
    "lr = 3e-4\n",
    "\n",
    "model = SequentialLinearModule(in_dim, out_dim, layer_sizes, activation, out_activation).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944ba137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialLinearModule(\n",
      "  (f): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=56, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=56, out_features=56, bias=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01)\n",
      "    (16): Linear(in_features=56, out_features=8, bias=True)\n",
      "    (17): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ff492",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4140e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        for j, (psi_true, theta_true) in enumerate(train_dataloader):\n",
    "            \n",
    "            theta_true = theta_true.type(torch.cuda.FloatTensor).to(device)\n",
    "            psi_true = psi_true.type(torch.cuda.FloatTensor).to(device)\n",
    "            theta_pred = model(psi_true)\n",
    "            loss = loss_function(theta_pred, theta_true)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        if not i%10:\n",
    "            print(f'epoch {i}, training loss: {running_loss/len(train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb6c504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss: 15.648360424041748\n",
      "epoch 10, training loss: 7.457572538852691\n",
      "epoch 20, training loss: 6.914449343681335\n",
      "epoch 30, training loss: 8.14009210586548\n",
      "epoch 40, training loss: 6.811366710662842\n",
      "epoch 50, training loss: 8.138894951343536\n",
      "epoch 60, training loss: 6.585956060886383\n",
      "epoch 70, training loss: 7.3378882455825805\n",
      "epoch 80, training loss: 8.699550716876983\n",
      "epoch 90, training loss: 6.757590854167939\n",
      "epoch 100, training loss: 7.280844666957855\n",
      "epoch 110, training loss: 7.3591417932510375\n",
      "epoch 120, training loss: 7.3494776082038875\n",
      "epoch 130, training loss: 6.708052275180816\n",
      "epoch 140, training loss: 6.694174191951752\n",
      "epoch 150, training loss: 6.6840537166595455\n",
      "epoch 160, training loss: 7.355789473056793\n",
      "epoch 170, training loss: 8.832986562252044\n",
      "epoch 180, training loss: 8.413821928501129\n",
      "epoch 190, training loss: 7.686001582145691\n",
      "epoch 200, training loss: 6.738543925285339\n",
      "epoch 210, training loss: 7.870853586196899\n",
      "epoch 220, training loss: 7.633511545658112\n",
      "epoch 230, training loss: 6.698164477348327\n",
      "epoch 240, training loss: 6.9759113073349\n",
      "epoch 250, training loss: 8.343655683994292\n",
      "epoch 260, training loss: 7.534496569633484\n",
      "epoch 270, training loss: 7.576939661502838\n",
      "epoch 280, training loss: 8.146783480644226\n",
      "epoch 290, training loss: 8.024112141132354\n",
      "epoch 300, training loss: 7.7331569027900695\n",
      "epoch 310, training loss: 7.853771598339081\n",
      "epoch 320, training loss: 7.867575209140778\n",
      "epoch 330, training loss: 8.036889755725861\n",
      "epoch 340, training loss: 7.3634414076805115\n",
      "epoch 350, training loss: 7.350204753875732\n",
      "epoch 360, training loss: 7.352984957695007\n",
      "epoch 370, training loss: 7.337741160392762\n",
      "epoch 380, training loss: 8.553046774864196\n",
      "epoch 390, training loss: 7.386759536266327\n",
      "epoch 400, training loss: 7.350764009952545\n",
      "epoch 410, training loss: 8.884154424667358\n",
      "epoch 420, training loss: 8.500045461654663\n",
      "epoch 430, training loss: 8.026032974720001\n",
      "epoch 440, training loss: 7.3668973350524904\n",
      "epoch 450, training loss: 7.347060735225678\n",
      "epoch 460, training loss: 6.685818159580231\n",
      "epoch 470, training loss: 6.733027844429016\n",
      "epoch 480, training loss: 7.364909753799439\n",
      "epoch 490, training loss: 6.878545095920563\n",
      "epoch 500, training loss: 7.690471177101135\n",
      "epoch 510, training loss: 7.366297104358673\n",
      "epoch 520, training loss: 7.411623735427856\n",
      "epoch 530, training loss: 6.835639114379883\n",
      "epoch 540, training loss: 6.716329572200775\n",
      "epoch 550, training loss: 7.040207140445709\n",
      "epoch 560, training loss: 7.886890342235565\n",
      "epoch 570, training loss: 7.561603672504425\n",
      "epoch 580, training loss: 8.043317811489105\n",
      "epoch 590, training loss: 8.19694531440735\n",
      "epoch 600, training loss: 7.355267832279205\n",
      "epoch 610, training loss: 7.028859844207764\n",
      "epoch 620, training loss: 7.466373438835144\n",
      "epoch 630, training loss: 8.399924137592315\n",
      "epoch 640, training loss: 8.048275792598725\n",
      "epoch 650, training loss: 7.645064260959625\n",
      "epoch 660, training loss: 6.684275453090668\n",
      "epoch 670, training loss: 6.698182241916657\n",
      "epoch 680, training loss: 6.6825013256073\n",
      "epoch 690, training loss: 6.686072297096253\n",
      "epoch 700, training loss: 6.68203152179718\n",
      "epoch 710, training loss: 6.680560517311096\n",
      "epoch 720, training loss: 6.679532988071442\n",
      "epoch 730, training loss: 7.3553839111328125\n",
      "epoch 740, training loss: 7.309315841197968\n",
      "epoch 750, training loss: 6.686633570194244\n",
      "epoch 760, training loss: 7.353938393592834\n",
      "epoch 770, training loss: 7.513358268737793\n",
      "epoch 780, training loss: 6.700506579875946\n",
      "epoch 790, training loss: 6.753069851398468\n",
      "epoch 800, training loss: 6.687330632209778\n",
      "epoch 810, training loss: 7.486562316417694\n",
      "epoch 820, training loss: 8.20650486946106\n",
      "epoch 830, training loss: 6.6898306941986085\n",
      "epoch 840, training loss: 6.776505439281464\n",
      "epoch 850, training loss: 6.69142796754837\n",
      "epoch 860, training loss: 7.733244004249573\n",
      "epoch 870, training loss: 7.663789267539978\n",
      "epoch 880, training loss: 7.352271230220794\n",
      "epoch 890, training loss: 6.684070675373078\n",
      "epoch 900, training loss: 7.400532391071319\n",
      "epoch 910, training loss: 7.120132231712342\n",
      "epoch 920, training loss: 7.438874318599701\n",
      "epoch 930, training loss: 7.365256221294403\n",
      "epoch 940, training loss: 7.479956111907959\n",
      "epoch 950, training loss: 7.546743974685669\n",
      "epoch 960, training loss: 7.205787801742554\n",
      "epoch 970, training loss: 6.704452662467957\n",
      "epoch 980, training loss: 7.324963719844818\n",
      "epoch 990, training loss: 6.794034395217896\n",
      "epoch 1000, training loss: 6.690445911884308\n",
      "epoch 1010, training loss: 7.374710409641266\n",
      "epoch 1020, training loss: 6.692809357643127\n",
      "epoch 1030, training loss: 6.688341116905212\n",
      "epoch 1040, training loss: 6.699246733188629\n",
      "epoch 1050, training loss: 6.692634007930756\n",
      "epoch 1060, training loss: 7.947443628311158\n",
      "epoch 1070, training loss: 6.719775221347809\n",
      "epoch 1080, training loss: 8.423894906044007\n",
      "epoch 1090, training loss: 7.207274341583252\n",
      "epoch 1100, training loss: 6.692327139377594\n",
      "epoch 1110, training loss: 7.3535140204429625\n",
      "epoch 1120, training loss: 6.831835079193115\n",
      "epoch 1130, training loss: 6.667440416812897\n",
      "epoch 1140, training loss: 8.907002847194672\n",
      "epoch 1150, training loss: 7.474047014713287\n",
      "epoch 1160, training loss: 6.687418355941772\n",
      "epoch 1170, training loss: 6.686721076965332\n",
      "epoch 1180, training loss: 6.680365881919861\n",
      "epoch 1190, training loss: 6.689941048622131\n",
      "epoch 1200, training loss: 6.684283692836761\n",
      "epoch 1210, training loss: 9.643348410129548\n",
      "epoch 1220, training loss: 7.49126948595047\n",
      "epoch 1230, training loss: 7.4094883561134335\n",
      "epoch 1240, training loss: 6.969796009063721\n",
      "epoch 1250, training loss: 6.727250556945801\n",
      "epoch 1260, training loss: 8.561820635795593\n",
      "epoch 1270, training loss: 6.6867280459403995\n",
      "epoch 1280, training loss: 6.961986203193664\n",
      "epoch 1290, training loss: 7.251202299594879\n",
      "epoch 1300, training loss: 6.69641765832901\n",
      "epoch 1310, training loss: 6.949407043457032\n",
      "epoch 1320, training loss: 7.070011889934539\n",
      "epoch 1330, training loss: 7.024310524463654\n",
      "epoch 1340, training loss: 8.449225840568543\n",
      "epoch 1350, training loss: 6.962093646526337\n",
      "epoch 1360, training loss: 7.434377887248993\n",
      "epoch 1370, training loss: 7.002412760257721\n",
      "epoch 1380, training loss: 7.736723549365998\n",
      "epoch 1390, training loss: 6.688341469764709\n",
      "epoch 1400, training loss: 6.801005530357361\n",
      "epoch 1410, training loss: 9.878250041007995\n",
      "epoch 1420, training loss: 6.9110307693481445\n",
      "epoch 1430, training loss: 7.897386186122894\n",
      "epoch 1440, training loss: 6.699753019809723\n",
      "epoch 1450, training loss: 8.59544443845749\n",
      "epoch 1460, training loss: 7.101897287368774\n",
      "epoch 1470, training loss: 8.010902383327483\n",
      "epoch 1480, training loss: 7.800143415927887\n",
      "epoch 1490, training loss: 6.714505312442779\n",
      "epoch 1500, training loss: 8.633307914733887\n",
      "epoch 1510, training loss: 7.961325988769532\n",
      "epoch 1520, training loss: 6.796721458435059\n",
      "epoch 1530, training loss: 7.38079336643219\n",
      "epoch 1540, training loss: 6.722088868618012\n",
      "epoch 1550, training loss: 6.78550354719162\n",
      "epoch 1560, training loss: 8.447006132602692\n",
      "epoch 1570, training loss: 7.099264566898346\n",
      "epoch 1580, training loss: 7.151949439048767\n",
      "epoch 1590, training loss: 7.986655516624451\n",
      "epoch 1600, training loss: 7.600338358879089\n",
      "epoch 1610, training loss: 7.519875020980835\n",
      "epoch 1620, training loss: 7.708105762004852\n",
      "epoch 1630, training loss: 8.082821815013885\n",
      "epoch 1640, training loss: 7.443822073936462\n",
      "epoch 1650, training loss: 7.750973360538483\n",
      "epoch 1660, training loss: 7.180351758003235\n",
      "epoch 1670, training loss: 6.746101725101471\n",
      "epoch 1680, training loss: 7.47320079088211\n",
      "epoch 1690, training loss: 7.011687524318695\n",
      "epoch 1700, training loss: 6.791981956958771\n",
      "epoch 1710, training loss: 6.872285351753235\n",
      "epoch 1720, training loss: 6.9276107621192935\n",
      "epoch 1730, training loss: 6.726030330657959\n",
      "epoch 1740, training loss: 6.833724853992462\n",
      "epoch 1750, training loss: 6.697443494796753\n",
      "epoch 1760, training loss: 7.155978903770447\n",
      "epoch 1770, training loss: 7.90512880563736\n",
      "epoch 1780, training loss: 6.98650402545929\n",
      "epoch 1790, training loss: 7.389527289867401\n",
      "epoch 1800, training loss: 6.739593880176544\n",
      "epoch 1810, training loss: 7.460443027019501\n",
      "epoch 1820, training loss: 7.059225661754608\n",
      "epoch 1830, training loss: 6.757110929489135\n",
      "epoch 1840, training loss: 7.3350196242332455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1850, training loss: 7.5508950066566465\n",
      "epoch 1860, training loss: 8.476918623447418\n",
      "epoch 1870, training loss: 7.663112421035766\n",
      "epoch 1880, training loss: 7.665670683383942\n",
      "epoch 1890, training loss: 7.2890850043296815\n",
      "epoch 1900, training loss: 7.479550640583039\n",
      "epoch 1910, training loss: 7.914831972122192\n",
      "epoch 1920, training loss: 8.352455449104308\n",
      "epoch 1930, training loss: 7.433028371334076\n",
      "epoch 1940, training loss: 7.6153326511383055\n",
      "epoch 1950, training loss: 7.4616093873977665\n",
      "epoch 1960, training loss: 6.816531982421875\n",
      "epoch 1970, training loss: 7.490635540485382\n",
      "epoch 1980, training loss: 6.846096849441528\n",
      "epoch 1990, training loss: 7.1934924101829525\n",
      "epoch 2000, training loss: 8.187871692180634\n",
      "epoch 2010, training loss: 8.738523564338685\n",
      "epoch 2020, training loss: 8.020571203231812\n",
      "epoch 2030, training loss: 7.520224890708923\n",
      "epoch 2040, training loss: 8.9087965798378\n",
      "epoch 2050, training loss: 8.227407355308532\n",
      "epoch 2060, training loss: 8.938180799484252\n",
      "epoch 2070, training loss: 7.939707925319672\n",
      "epoch 2080, training loss: 6.994399485588073\n",
      "epoch 2090, training loss: 6.843149955272675\n",
      "epoch 2100, training loss: 6.942505393028259\n",
      "epoch 2110, training loss: 6.708971629142761\n",
      "epoch 2120, training loss: 7.78923024892807\n",
      "epoch 2130, training loss: 7.509243030548095\n",
      "epoch 2140, training loss: 7.941305849552155\n",
      "epoch 2150, training loss: 8.468504314422608\n",
      "epoch 2160, training loss: 8.45347312450409\n",
      "epoch 2170, training loss: 7.007234437465668\n",
      "epoch 2180, training loss: 7.189517567157745\n",
      "epoch 2190, training loss: 7.532878148555755\n",
      "epoch 2200, training loss: 7.538058524131775\n",
      "epoch 2210, training loss: 6.712287678718567\n",
      "epoch 2220, training loss: 10.157670521736145\n",
      "epoch 2230, training loss: 7.121029577255249\n",
      "epoch 2240, training loss: 6.791742415428161\n",
      "epoch 2250, training loss: 6.719319686889649\n",
      "epoch 2260, training loss: 6.75311625957489\n",
      "epoch 2270, training loss: 7.140817286968232\n",
      "epoch 2280, training loss: 7.455968151092529\n",
      "epoch 2290, training loss: 6.704695041179657\n",
      "epoch 2300, training loss: 7.2568483996391295\n",
      "epoch 2310, training loss: 8.635961763858795\n",
      "epoch 2320, training loss: 8.536790740489959\n",
      "epoch 2330, training loss: 6.866276483535767\n",
      "epoch 2340, training loss: 7.046409339904785\n",
      "epoch 2350, training loss: 6.724418215751648\n",
      "epoch 2360, training loss: 6.7318525314331055\n",
      "epoch 2370, training loss: 6.826776289939881\n",
      "epoch 2380, training loss: 7.250007853507996\n",
      "epoch 2390, training loss: 6.744067053794861\n",
      "epoch 2400, training loss: 6.975879812240601\n",
      "epoch 2410, training loss: 6.690822474956512\n",
      "epoch 2420, training loss: 7.275772581100464\n",
      "epoch 2430, training loss: 7.426036491394043\n",
      "epoch 2440, training loss: 7.242143702507019\n",
      "epoch 2450, training loss: 7.4080785727500915\n",
      "epoch 2460, training loss: 7.119619188308715\n",
      "epoch 2470, training loss: 6.735787949562073\n",
      "epoch 2480, training loss: 7.113365287780762\n",
      "epoch 2490, training loss: 7.73355357170105\n",
      "epoch 2500, training loss: 7.374045979976654\n",
      "epoch 2510, training loss: 7.410182490348816\n",
      "epoch 2520, training loss: 6.874989626407623\n",
      "epoch 2530, training loss: 7.201916623115539\n",
      "epoch 2540, training loss: 7.510667192935943\n",
      "epoch 2550, training loss: 6.74449946641922\n",
      "epoch 2560, training loss: 6.71059944152832\n",
      "epoch 2570, training loss: 8.15308913230896\n",
      "epoch 2580, training loss: 6.823739376068115\n",
      "epoch 2590, training loss: 8.123773322105407\n",
      "epoch 2600, training loss: 8.128833892345428\n",
      "epoch 2610, training loss: 6.856903977394104\n",
      "epoch 2620, training loss: 7.1242989778518675\n",
      "epoch 2630, training loss: 7.47715405702591\n",
      "epoch 2640, training loss: 7.356438374519348\n",
      "epoch 2650, training loss: 8.833450472354889\n",
      "epoch 2660, training loss: 7.6424755239486695\n",
      "epoch 2670, training loss: 7.965296449661255\n",
      "epoch 2680, training loss: 7.47229834318161\n",
      "epoch 2690, training loss: 6.827585918903351\n",
      "epoch 2700, training loss: 7.417120048999786\n",
      "epoch 2710, training loss: 7.628460981845856\n",
      "epoch 2720, training loss: 7.339246401786804\n",
      "epoch 2730, training loss: 6.992959570884705\n",
      "epoch 2740, training loss: 6.838565475940705\n",
      "epoch 2750, training loss: 6.780285747051239\n",
      "epoch 2760, training loss: 7.495205357074737\n",
      "epoch 2770, training loss: 7.545441496372223\n",
      "epoch 2780, training loss: 7.103823838233947\n",
      "epoch 2790, training loss: 6.694203162193299\n",
      "epoch 2800, training loss: 6.7013365530967715\n",
      "epoch 2810, training loss: 6.803455600738525\n",
      "epoch 2820, training loss: 7.327699828147888\n",
      "epoch 2830, training loss: 6.804510569572448\n",
      "epoch 2840, training loss: 7.2329558396339415\n",
      "epoch 2850, training loss: 7.545310657024384\n",
      "epoch 2860, training loss: 7.436242432594299\n",
      "epoch 2870, training loss: 7.23172756433487\n",
      "epoch 2880, training loss: 8.016852869987488\n",
      "epoch 2890, training loss: 7.928087296485901\n",
      "epoch 2900, training loss: 7.4942604804039\n",
      "epoch 2910, training loss: 6.954230711460114\n",
      "epoch 2920, training loss: 7.49497504234314\n",
      "epoch 2930, training loss: 7.430845594406128\n",
      "epoch 2940, training loss: 7.5789787435531615\n",
      "epoch 2950, training loss: 8.280773639678955\n",
      "epoch 2960, training loss: 8.435859291553497\n",
      "epoch 2970, training loss: 8.458259832859039\n",
      "epoch 2980, training loss: 8.712368392944336\n",
      "epoch 2990, training loss: 8.421803085803985\n",
      "epoch 3000, training loss: 9.26696175813675\n",
      "epoch 3010, training loss: 8.28342656135559\n",
      "epoch 3020, training loss: 8.324772441387177\n",
      "epoch 3030, training loss: 7.724069898128509\n",
      "epoch 3040, training loss: 9.774362869262696\n",
      "epoch 3050, training loss: 7.044118251800537\n",
      "epoch 3060, training loss: 7.387371697425842\n",
      "epoch 3070, training loss: 7.102335293292999\n",
      "epoch 3080, training loss: 6.760258960723877\n",
      "epoch 3090, training loss: 8.465790362358094\n",
      "epoch 3100, training loss: 8.240782771110535\n",
      "epoch 3110, training loss: 6.8757697057724\n",
      "epoch 3120, training loss: 6.7049820566177365\n",
      "epoch 3130, training loss: 6.909831960201263\n",
      "epoch 3140, training loss: 7.147988646030426\n",
      "epoch 3150, training loss: 6.717548937797546\n",
      "epoch 3160, training loss: 7.8474509501457215\n",
      "epoch 3170, training loss: 7.943726253509522\n",
      "epoch 3180, training loss: 7.540326209068298\n",
      "epoch 3190, training loss: 8.359547548294067\n",
      "epoch 3200, training loss: 7.658204593658447\n",
      "epoch 3210, training loss: 6.722379853725434\n",
      "epoch 3220, training loss: 8.074525318145753\n",
      "epoch 3230, training loss: 8.170898044109345\n",
      "epoch 3240, training loss: 7.691558640003205\n",
      "epoch 3250, training loss: 6.88067110300064\n",
      "epoch 3260, training loss: 7.001184611320496\n",
      "epoch 3270, training loss: 8.284361569881439\n",
      "epoch 3280, training loss: 7.668636093139648\n",
      "epoch 3290, training loss: 7.139553687572479\n",
      "epoch 3300, training loss: 7.503636889457702\n",
      "epoch 3310, training loss: 6.795131573677063\n",
      "epoch 3320, training loss: 8.024733791351318\n",
      "epoch 3330, training loss: 6.767340617179871\n",
      "epoch 3340, training loss: 8.386870183944701\n",
      "epoch 3350, training loss: 7.650234370231629\n",
      "epoch 3360, training loss: 7.041980893611908\n",
      "epoch 3370, training loss: 6.797197782993317\n",
      "epoch 3380, training loss: 8.323995342254639\n",
      "epoch 3390, training loss: 7.268044331073761\n",
      "epoch 3400, training loss: 7.209281411170959\n",
      "epoch 3410, training loss: 6.932075219154358\n",
      "epoch 3420, training loss: 7.454838252067566\n",
      "epoch 3430, training loss: 6.959780826568603\n",
      "epoch 3440, training loss: 7.324193923473358\n",
      "epoch 3450, training loss: 8.085798597335815\n",
      "epoch 3460, training loss: 7.169850161075592\n",
      "epoch 3470, training loss: 7.027874286174774\n",
      "epoch 3480, training loss: 6.85600049495697\n",
      "epoch 3490, training loss: 6.716578547954559\n",
      "epoch 3500, training loss: 6.753201842308044\n",
      "epoch 3510, training loss: 8.08975617647171\n",
      "epoch 3520, training loss: 7.7365008425712585\n",
      "epoch 3530, training loss: 6.703539941310883\n",
      "epoch 3540, training loss: 6.710328245162964\n",
      "epoch 3550, training loss: 7.298462603092194\n",
      "epoch 3560, training loss: 10.158660733699799\n",
      "epoch 3570, training loss: 7.87634304523468\n",
      "epoch 3580, training loss: 7.345836508274078\n",
      "epoch 3590, training loss: 7.549811747074127\n",
      "epoch 3600, training loss: 6.723114001750946\n",
      "epoch 3610, training loss: 6.81220513343811\n",
      "epoch 3620, training loss: 6.692615690231324\n",
      "epoch 3630, training loss: 7.481099383831024\n",
      "epoch 3640, training loss: 6.8033935570716855\n",
      "epoch 3650, training loss: 8.505591442584992\n",
      "epoch 3660, training loss: 7.172859308719635\n",
      "epoch 3670, training loss: 8.988515794277191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3680, training loss: 6.895452461242676\n",
      "epoch 3690, training loss: 6.815780580043793\n",
      "epoch 3700, training loss: 6.97844144821167\n",
      "epoch 3710, training loss: 7.070657095909119\n",
      "epoch 3720, training loss: 6.748726511001587\n",
      "epoch 3730, training loss: 6.711070663928986\n",
      "epoch 3740, training loss: 8.303292846679687\n",
      "epoch 3750, training loss: 7.179029858112335\n",
      "epoch 3760, training loss: 6.711044554710388\n",
      "epoch 3770, training loss: 6.721805703639984\n",
      "epoch 3780, training loss: 6.739846451282501\n",
      "epoch 3790, training loss: 7.007671463489532\n",
      "epoch 3800, training loss: 6.674271309375763\n",
      "epoch 3810, training loss: 6.831736817359924\n",
      "epoch 3820, training loss: 7.2285942721366885\n",
      "epoch 3830, training loss: 7.64167130947113\n",
      "epoch 3840, training loss: 6.92780202627182\n",
      "epoch 3850, training loss: 7.399382064342499\n",
      "epoch 3860, training loss: 7.961297125816345\n",
      "epoch 3870, training loss: 6.995720822811126\n",
      "epoch 3880, training loss: 6.964945685863495\n",
      "epoch 3890, training loss: 7.0157902050018315\n",
      "epoch 3900, training loss: 7.02682502746582\n",
      "epoch 3910, training loss: 7.047195825576782\n",
      "epoch 3920, training loss: 6.882768607139587\n",
      "epoch 3930, training loss: 6.692050375938416\n",
      "epoch 3940, training loss: 7.364864227771759\n",
      "epoch 3950, training loss: 7.361943762302399\n",
      "epoch 3960, training loss: 7.385241413116455\n",
      "epoch 3970, training loss: 7.356254179477691\n",
      "epoch 3980, training loss: 7.352390916347503\n",
      "epoch 3990, training loss: 7.380170633792877\n",
      "epoch 4000, training loss: 7.351992380619049\n",
      "epoch 4010, training loss: 8.060644943714141\n",
      "epoch 4020, training loss: 6.7174769115448\n",
      "epoch 4030, training loss: 7.357269790172577\n",
      "epoch 4040, training loss: 7.358765525817871\n",
      "epoch 4050, training loss: 6.699752867221832\n",
      "epoch 4060, training loss: 8.196431777477265\n",
      "epoch 4070, training loss: 7.321576318740845\n",
      "epoch 4080, training loss: 7.356555004119873\n",
      "epoch 4090, training loss: 6.789193236827851\n",
      "epoch 4100, training loss: 6.7261325860023495\n",
      "epoch 4110, training loss: 7.661981585025788\n",
      "epoch 4120, training loss: 7.683964886665344\n",
      "epoch 4130, training loss: 6.824230425357818\n",
      "epoch 4140, training loss: 6.70603458404541\n",
      "epoch 4150, training loss: 6.700669023990631\n",
      "epoch 4160, training loss: 6.703465526103973\n",
      "epoch 4170, training loss: 8.142717509269714\n",
      "epoch 4180, training loss: 7.36753176689148\n",
      "epoch 4190, training loss: 7.357858250141144\n",
      "epoch 4200, training loss: 6.743800041675567\n",
      "epoch 4210, training loss: 8.038337013721467\n",
      "epoch 4220, training loss: 7.4519452023506165\n",
      "epoch 4230, training loss: 8.363550574779511\n",
      "epoch 4240, training loss: 7.3494684743881225\n",
      "epoch 4250, training loss: 7.3801284241676335\n",
      "epoch 4260, training loss: 7.4035922813415525\n",
      "epoch 4270, training loss: 6.779050080776215\n",
      "epoch 4280, training loss: 7.174050490856171\n",
      "epoch 4290, training loss: 7.179791829586029\n",
      "epoch 4300, training loss: 7.072855849266052\n",
      "epoch 4310, training loss: 6.83483409166336\n",
      "epoch 4320, training loss: 7.132535991668701\n",
      "epoch 4330, training loss: 6.711883530616761\n",
      "epoch 4340, training loss: 7.007323114871979\n",
      "epoch 4350, training loss: 6.720139663219452\n",
      "epoch 4360, training loss: 6.756458992958069\n",
      "epoch 4370, training loss: 7.4133696103096005\n",
      "epoch 4380, training loss: 6.697494101524353\n",
      "epoch 4390, training loss: 7.050267994403839\n",
      "epoch 4400, training loss: 7.591966142654419\n",
      "epoch 4410, training loss: 8.143066084384918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33268/3237553118.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33268/1726490247.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\QITE\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\QITE\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\QITE\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "train(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe576577",
   "metadata": {},
   "source": [
    "# Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf5b7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "pair = my_ansatz.generate_pairs(1)\n",
    "psi_true = torch.FloatTensor(pair[0])\n",
    "theta_true = torch.FloatTensor(pair[1])\n",
    "\n",
    "#psi_true, theta_true = next(iter(train_dataloader))\n",
    "\n",
    "theta_true = theta_true.type(torch.cuda.FloatTensor).to(device)\n",
    "psi_true = psi_true.type(torch.cuda.FloatTensor).to(device)\n",
    "\n",
    "theta_pred = model(psi_true)\n",
    "psi_pred = my_ansatz.apply_ansatz(theta_pred.cpu().detach().numpy()[0])\n",
    "\n",
    "psi_true = psi_true.cpu().detach().numpy()[0]\n",
    "theta_pred = theta_pred.cpu().detach().numpy()[0]\n",
    "theta_true = theta_true.cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ad1377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.87727344, -1.9147718 , -1.3180441 ,  2.3590467 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233d09ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7401611 , -0.78566843, -0.00961818,  0.00991248], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65833091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5211627 , -0.34727734, -0.7402801 ,  0.24448584], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0cfe360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86128301, -0.13845813, -0.35689147,  0.33413978])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3078559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac08da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
